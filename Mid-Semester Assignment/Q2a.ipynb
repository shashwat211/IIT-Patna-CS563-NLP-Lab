{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Required Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import string\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from nltk.util import ngrams\n",
    "from tabulate import tabulate\n",
    "from collections import defaultdict, Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuations(data):\n",
    "    result = [line.translate(str.maketrans('', '', string.punctuation)) for line in data]\n",
    "    return result\n",
    "\n",
    "def load_dataset():\n",
    "    data_file = open('penn-data.json')\n",
    "    dataset = np.array(json.load(data_file), dtype=object)\n",
    "    x, y = remove_punctuations(dataset[:, 0]), dataset[:, 1]\n",
    "    return [[(word, tag) for word, tag in zip(sentence.split(), tag_seq)] for sentence, tag_seq in zip(x, y)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset()\n",
    "train, test = train_test_split(dataset, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hidden Markov Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HMM:\n",
    "    LAPLACE = 0.0000001\n",
    "    def __init__(self, train, ngram=2, with_context=False):\n",
    "        self.state_list = ['*'] + list(set([token[1] for sequence in train for token in sequence])) + ['STOP']\n",
    "        self.transition = self.generate_transition_matrix(train, ngram=ngram)\n",
    "        self.emission = self.generate_emission_matrix(train, with_context=with_context)\n",
    "    \n",
    "    def generate_transition_matrix(self, train, ngram=2, laplace_factor=LAPLACE):\n",
    "        y = [[token[1] for token in sequence] for sequence in train]\n",
    "        ngram_tags = []\n",
    "        for tag_list in y:\n",
    "            tag_list = [\"*\"] * (ngram - 1) + tag_list + [\"STOP\"]\n",
    "            ngram_tags.extend(ngrams(tag_list, ngram))\n",
    "        ngram_count = dict(Counter(ngram_tags))\n",
    "\n",
    "        n_minus_1_gram_tags = []\n",
    "        for tag_list in y:\n",
    "            tag_list = [\"*\"] * (ngram - 1) + tag_list + [\"STOP\"]\n",
    "            n_minus_1_gram_tags.extend(ngrams(tag_list, ngram - 1))\n",
    "        n_minus_1_gram_count = dict(Counter(n_minus_1_gram_tags))\n",
    "\n",
    "        transition_matrix = defaultdict(lambda: laplace_factor)\n",
    "\n",
    "        for ngram_tuple in ngram_count:\n",
    "            n_minus_1_gram_tuple = ngram_tuple[:-1]\n",
    "            transition_matrix[ngram_tuple] = ngram_count[ngram_tuple] / n_minus_1_gram_count[n_minus_1_gram_tuple]\n",
    "\n",
    "        return transition_matrix\n",
    "\n",
    "    def generate_emission_matrix(self, train, with_context=False, laplace_factor=LAPLACE):\n",
    "        x = [[token[0] for token in sequence] for sequence in train]\n",
    "        y = [[token[1] for token in sequence] for sequence in train]\n",
    "        word_tag_count = defaultdict(lambda: 0)\n",
    "        tag_count = defaultdict(lambda: 0)\n",
    "\n",
    "        for line, tags in zip(x, y):\n",
    "            prev_tag = '*'\n",
    "            for word, tag in zip(line, tags):\n",
    "                if with_context:\n",
    "                    tag_count[(tag, prev_tag)] += 1\n",
    "                    word_tag_count[(word, tag, prev_tag)] += 1\n",
    "                else:\n",
    "                    tag_count[(tag,)] += 1\n",
    "                    word_tag_count[(word, tag)] += 1\n",
    "                prev_tag = tag\n",
    "                \n",
    "        \n",
    "        emission_matrix = defaultdict(lambda: laplace_factor)\n",
    "        \n",
    "        for word_tags in word_tag_count.keys():\n",
    "            tags = word_tags[1:]\n",
    "            emission_matrix[word_tags] = word_tag_count[word_tags] / tag_count[tags]\n",
    "\n",
    "        return emission_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Viterbi Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kappa(position, state_list):\n",
    "    return state_list if position not in [0, -1] else ['*']\n",
    "\n",
    "def viterbi_trigram(sentence, hmm, with_context=False):\n",
    "    pi = defaultdict(lambda: 0)\n",
    "    bp = defaultdict(lambda: \"OTH\")\n",
    "    pi[(0, '*', '*')] = 1.0\n",
    "    A = hmm.transition\n",
    "    B = hmm.emission\n",
    "    state_list = hmm.state_list\n",
    "\n",
    "    n = len(sentence)\n",
    "\n",
    "    for k in range(1, n + 1):\n",
    "        u_set = kappa(k - 1, state_list)\n",
    "        v_set = kappa(k, state_list)\n",
    "        w_set = kappa(k - 2, state_list)\n",
    "\n",
    "        for v in v_set:\n",
    "            for u in u_set:\n",
    "                for w in w_set:\n",
    "                    if with_context:\n",
    "                        emission_tuple = (sentence[k - 1], v, u)\n",
    "                    else:\n",
    "                        emission_tuple = (sentence[k - 1], v)\n",
    "                    reach_prob = pi[(k - 1, w, u)] * A[(w, u, v)] * B[emission_tuple]\n",
    "                    if reach_prob > pi[(k, u, v)]:\n",
    "                        pi[(k, u, v)] = reach_prob\n",
    "                        bp[(k, u, v)] = w\n",
    "    \n",
    "    u_set = kappa(n - 1, state_list)\n",
    "    v_set = kappa(n, state_list)\n",
    "    result_tags = []\n",
    "    for u in u_set:\n",
    "        for v in v_set:\n",
    "            if len(result_tags) == 0:\n",
    "                result_tags = [v, u]\n",
    "            if pi[(n, u, v)] * A[(u, v, 'STOP')] > \\\n",
    "            pi[(n, result_tags[1], result_tags[0])] * A[result_tags[1], result_tags[0], 'STOP']:\n",
    "                result_tags = [v, u]\n",
    "    \n",
    "    if n == 0:\n",
    "        return []\n",
    "    elif n == 1:\n",
    "        return [result_tags[0]]\n",
    "    \n",
    "    for k in range(n - 2, 0, -1):\n",
    "        result_tags.append(bp[(k + 2, result_tags[-1], result_tags[-2])])\n",
    "    \n",
    "    result_tags.reverse()\n",
    "\n",
    "    return result_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_metrics(true, pred):\n",
    "    true = [ch for word in true for ch in word]\n",
    "    pred = [ch for word in pred for ch in word]\n",
    "    classes = list(set(true))\n",
    "    classes.sort()\n",
    "    # accuracy: (tp + tn) / (p + n)\n",
    "    acc = accuracy_score(true, pred)\n",
    "    # precision tp / (tp + fp)\n",
    "    precision = precision_score(true, pred, average=None)\n",
    "    # recall: tp / (tp + fn)\n",
    "    recall = recall_score(true, pred, average=None)\n",
    "    # f1: 2 tp / (2 tp + fp + fn)\n",
    "    f1 = f1_score(true, pred, average=None)\n",
    "\n",
    "    return acc, precision, recall, f1, classes\n",
    "\n",
    "def print_metrics(test_acc, precision, recall, f1, classes):\n",
    "    print(f\"Accuracy of the model: {test_acc}\")\n",
    "    print(tabulate(zip(classes, precision, recall, f1),\n",
    "                   headers=['Class (Alphabet)', 'Precision', 'Recall', 'F1'],\n",
    "                   tablefmt='orgtbl'))\n",
    "\n",
    "def test_and_evaluate(hmm, test_data):\n",
    "    test = [[token[0] for token in sequence] for sequence in test_data]\n",
    "    true = [[token[1] for token in sequence] for sequence in test_data]\n",
    "    pred = []\n",
    "\n",
    "    for sentence in tqdm(test, total=len(test)):\n",
    "        pred.append(viterbi_trigram(sentence, hmm, with_context=True))\n",
    "\n",
    "    accuracy, precision, recall, f1, classes = evaluate_metrics(true, pred)\n",
    "    print_metrics(accuracy, precision, recall, f1, classes)        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 783/783 [19:58<00:00,  1.53s/it]\n",
      "/home/shashwat211/miniconda3/envs/nlp-midsem/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/shashwat211/miniconda3/envs/nlp-midsem/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model: 0.7687766892740517\n",
      "| Class (Alphabet)   |   Precision |    Recall |       F1 |\n",
      "|--------------------+-------------+-----------+----------|\n",
      "| ''                 |   0         | 0         | 0        |\n",
      "| -LRB-              |   0         | 0         | 0        |\n",
      "| -RRB-              |   0.652174  | 0.535714  | 0.588235 |\n",
      "| :                  |   0.619048  | 0.393939  | 0.481481 |\n",
      "| CC                 |   0.0357143 | 0.0192308 | 0.025    |\n",
      "| CD                 |   0.794805  | 0.72      | 0.755556 |\n",
      "| DT                 |   0.813031  | 0.790634  | 0.801676 |\n",
      "| EX                 |   0.789119  | 0.881027  | 0.832544 |\n",
      "| FW                 |   0.909091  | 0.526316  | 0.666667 |\n",
      "| IN                 |   0         | 0         | 0        |\n",
      "| JJ                 |   0.774687  | 0.870313  | 0.81972  |\n",
      "| JJR                |   0.677768  | 0.683303  | 0.680524 |\n",
      "| JJS                |   0.661538  | 0.544304  | 0.597222 |\n",
      "| LS                 |   0.814815  | 0.578947  | 0.676923 |\n",
      "| MD                 |   0         | 0         | 0        |\n",
      "| NN                 |   0.855615  | 0.91954   | 0.886427 |\n",
      "| NNP                |   0.759666  | 0.794923  | 0.776895 |\n",
      "| NNPS               |   0.825058  | 0.814709  | 0.819851 |\n",
      "| NNS                |   0.666667  | 0.444444  | 0.533333 |\n",
      "| PDT                |   0.78481   | 0.710311  | 0.745704 |\n",
      "| PRP                |   0.333333  | 0.4       | 0.363636 |\n",
      "| PRP$               |   0.765333  | 0.864458  | 0.811881 |\n",
      "| RB                 |   0.734848  | 0.713235  | 0.723881 |\n",
      "| RBR                |   0.713974  | 0.626437  | 0.667347 |\n",
      "| RBS                |   0.425     | 0.62963   | 0.507463 |\n",
      "| RP                 |   1         | 0.285714  | 0.444444 |\n",
      "| TO                 |   0.533333  | 0.615385  | 0.571429 |\n",
      "| UH                 |   0.847458  | 0.797267  | 0.821596 |\n",
      "| VB                 |   0         | 0         | 0        |\n",
      "| VBD                |   0.798354  | 0.762279  | 0.779899 |\n",
      "| VBG                |   0.754685  | 0.738333  | 0.74642  |\n",
      "| VBN                |   0.666667  | 0.397351  | 0.497925 |\n",
      "| VBP                |   0.657534  | 0.657534  | 0.657534 |\n",
      "| VBZ                |   0.701681  | 0.668     | 0.684426 |\n",
      "| WDT                |   0.858726  | 0.786802  | 0.821192 |\n",
      "| WP                 |   0.795181  | 0.75      | 0.77193  |\n",
      "| WP$                |   0.871795  | 0.62963   | 0.731183 |\n",
      "| WRB                |   1         | 0.25      | 0.4      |\n"
     ]
    }
   ],
   "source": [
    "hmm = HMM(train, ngram=3, with_context=True)\n",
    "test_and_evaluate(hmm, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working on the given case of POS Tagging\n",
    "We run the viterbi algorithm on the given sentence using the parameters learned through the Hidden Markov Model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That_/DT former_/JJ Sri_/NN Lanka_/IN skipper_/NNP and_/CC ace_/NNP batsman_/NNP Aravinda_/NNP De_/NNP Silva_/NNP is_/VBZ a_/DT man_/NN of_/JJ few_/JJ words_/NN was_/NN very_/RB much_/JJ evident_/NN on_/IN Wednesday_/NNP when_/WRB the_/DT legendary_/NNP batsman_/NNP ,_/NNP who_/WP has_/VBZ always_/RB let_/VBN his_/PRP$ bat_/JJ talk_/NN ,_/CC struggled_/VBD to_/TO answer_/VB a_/DT barrage_/JJ of_/NN questions_/NNS at_/IN a_/DT function_/NN to_F_/TO promote_/VB "
     ]
    }
   ],
   "source": [
    "sentence = \"That former Sri Lanka skipper and ace batsman Aravinda De Silva is a man of few\\\n",
    "    words was very much evident on Wednesday when the legendary batsman , who has\\\n",
    "        always let his bat talk , struggled to answer a barrage of questions at a function to_F\\\n",
    "            promote\".split()\n",
    "\n",
    "predicted_tags = viterbi_trigram(sentence, hmm, with_context=True)\n",
    "for word, tag in zip(sentence, predicted_tags):\n",
    "    print(f'{word}_/{tag}', end=' ')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "46af86b762384d5b484041a46ecc7133789ecef2b2e62713afc23563f4b0cd95"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('nlp-midsem')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
