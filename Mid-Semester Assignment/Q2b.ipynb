{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modifying the Viterbi Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Required Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "START_SYMBOL = '*'\n",
    "STOP_SYMBOL = 'STOP'\n",
    "RARE_SYMBOL = '_RARE_'\n",
    "RARE_WORD_MAX_FREQ = 5\n",
    "LOG_PROB_OF_ZERO = -1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Receives a list of tagged sentences and processes each sentence to generate a list of words and a list of tags.\n",
    "# Each sentence is a string of space separated \"WORD/TAG\" tokens, with a newline character in the end.\n",
    "# Remember to include start and stop symbols in yout returned lists, as defined by the constants START_SYMBOL and STOP_SYMBOL.\n",
    "# penn_words (the list of words) should be a list where every element is a list of the tags of a particular sentence.\n",
    "# penn_tags (the list of tags) should be a list where every element is a list of the tags of a particular sentence.\n",
    "def read_data(data):\n",
    "    all_words = []\n",
    "    all_tags = []\n",
    "    for sentence in data:\n",
    "        words = sentence[0].split(' ')\n",
    "        words = [START_SYMBOL, START_SYMBOL] + words + [STOP_SYMBOL]\n",
    "        tags = [START_SYMBOL, START_SYMBOL] + sentence[1] + [STOP_SYMBOL]\n",
    "        assert len(words) == len(tags)\n",
    "        all_words.append(words)\n",
    "        all_tags.append(tags)\n",
    "    return all_words, all_tags\n",
    "\n",
    "# This function takes tags from the training data and calculates tag trigram probabilities.\n",
    "# It returns a python dictionary where the keys are tuples that represent the tag trigram, and the values are the log probability of that trigram\n",
    "def get_bigrams(item):\n",
    "    bigrams_tmp = []\n",
    "    for i in range(len(item)-1):\n",
    "        bigrams_tmp.append((item[i], item[i+1]))\n",
    "    return bigrams_tmp\n",
    "\n",
    "def get_trigrams(item):\n",
    "    trirams_tmp = []\n",
    "    for i in range(len(item)-2):\n",
    "        trirams_tmp.append((item[i], item[i+1], item[i+2]))\n",
    "    return trirams_tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hidden Markov Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transitions_probs(penn_tags):\n",
    "    transition_values = {}\n",
    "\n",
    "    bigram_count = {}\n",
    "    trigram_count = {}\n",
    "\n",
    "    for item in penn_tags:\n",
    "        bigram_tmp = get_bigrams(item)\n",
    "        trigram_tmp = get_trigrams(item)\n",
    "\n",
    "        for bigram in bigram_tmp:\n",
    "            if bigram in bigram_count:\n",
    "                bigram_count[bigram] += 1\n",
    "            else:\n",
    "                bigram_count[bigram] = 1\n",
    "\n",
    "        for trigram in trigram_tmp:\n",
    "            if trigram in trigram_count:\n",
    "                trigram_count[trigram] += 1\n",
    "            else:\n",
    "                trigram_count[trigram] =1\n",
    "\n",
    "    for trigram in trigram_count:\n",
    "        transition_values[trigram] = math.log(trigram_count[trigram], 2) - math.log(bigram_count[trigram[:2]],2)\n",
    "    return transition_values\n",
    "\n",
    "# This function takes output from get_transitions_probs() and outputs it in the proper format\n",
    "def s2_output(transition_values, filename):\n",
    "    outfile = open(filename, \"w\")\n",
    "    trigrams = transition_values.keys()\n",
    "    trigrams = sorted(trigrams)  \n",
    "    for trigram in trigrams:\n",
    "        output = \" \".join(['TRIGRAM', trigram[0], trigram[1], trigram[2], str(transition_values[trigram])])\n",
    "        outfile.write(output + '\\n')\n",
    "    outfile.close()\n",
    "\n",
    "# Takes the words from the training data and returns a set of all of the words that occur more than 5 times (use RARE_WORD_MAX_FREQ)\n",
    "# penn_words is a python list where every element is a python list of the words of a particular sentence.\n",
    "# Note: words that appear exactly 5 times should be considered rare!\n",
    "def calc_known(penn_words):\n",
    "    known_words = set([])\n",
    "    \n",
    "    word_count = {}\n",
    "\n",
    "    for item in penn_words:\n",
    "        for word in item:\n",
    "            if word in word_count:\n",
    "                word_count[word] += 1\n",
    "            else:\n",
    "                word_count[word] = 1\n",
    "\n",
    "    for item in word_count:\n",
    "        if word_count[item] > RARE_WORD_MAX_FREQ:\n",
    "            known_words.add(item)\n",
    "\t    \n",
    "    return known_words\n",
    "\n",
    "# Takes the words from the training data and a set of words that should not be replaced for '_RARE_'\n",
    "# Returns the equivalent to penn_words but replacing the unknown words by '_RARE_' (use RARE_SYMBOL constant)\n",
    "def replace_rare(penn_words, known_words):\n",
    "    penn_words_rare = []\n",
    "\n",
    "    for item in penn_words:\n",
    "        tmp = []\n",
    "        for word in item:\n",
    "            if word in known_words:\n",
    "                tmp.append(word)\n",
    "            else:\n",
    "                tmp.append(RARE_SYMBOL)\n",
    "        penn_words_rare.append(tmp)\n",
    "\n",
    "    return penn_words_rare\n",
    "\n",
    "# This function takes the ouput from replace_rare and outputs it to a file\n",
    "def s3_output(rare, filename):\n",
    "    outfile = open(filename, 'w')\n",
    "    for sentence in rare:\n",
    "        outfile.write(' '.join(sentence[2:-1]) + '\\n')\n",
    "    outfile.close()\n",
    "\n",
    "# Calculates emission probabilities and creates a set of all possible tags\n",
    "# The first return value is a python dictionary where each key is a tuple in which the first element is a word\n",
    "# and the second is a tag, and the value is the log probability of the emission of the word given the tag\n",
    "# The second return value is a set of all possible tags for this data set\n",
    "def get_emission_probs(penn_words_rare, penn_tags):\n",
    "    e_values = {}\n",
    "    taglist = set([])\n",
    "\n",
    "    tag_count = {}\n",
    "    word_tag_count = {}\n",
    "    for i in range(len(penn_tags)):\n",
    "        sentence = penn_words_rare[i]\n",
    "        tags = penn_tags[i]\n",
    "        for j in range(1, len(tags)):\n",
    "            word = sentence[j]\n",
    "            tag = tags[j]\n",
    "            tag_prev = tags[j-1]\n",
    "            if (word, tag, tag_prev) in word_tag_count:\n",
    "                word_tag_count[(word,tag,tag_prev)] += 1\n",
    "            else:\n",
    "                word_tag_count[(word,tag,tag_prev)] =1\n",
    "            if (tag, tag_prev) in tag_count:\n",
    "                tag_count[tag, tag_prev] += 1\n",
    "            else:\n",
    "                tag_count[tag, tag_prev] = 1\n",
    "\n",
    "    for item in word_tag_count:\n",
    "        e_values[item] = math.log(word_tag_count[item],2) - math.log(tag_count[item[1:]],2)\n",
    "\n",
    "    for item in tag_count:\n",
    "        taglist.add(item[0])\n",
    "        taglist.add(item[1])\n",
    "\n",
    "    return e_values, taglist\n",
    "\n",
    "# This function takes the output from calc_emissions() and outputs it\n",
    "def s4_output(e_values, filename):\n",
    "    outfile = open(filename, \"w\")\n",
    "    emissions = e_values.keys()\n",
    "    emissions = sorted(emissions)  \n",
    "    for item in emissions:\n",
    "        output = \" \".join([item[0], item[1], item[2], str(e_values[item])])\n",
    "        outfile.write(output + '\\n')\n",
    "    outfile.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Modified Viterbi Algorithm\n",
    "This function takes data to tag (penn_dev_words), a set of all possible tags (taglist), a set of all known words (known_words), trigram probabilities (transition_values) and emission probabilities (e_values) and outputs a list where every element is a tagged sentence (in the WORD_/TAG format, separated by spaces and with a newline in the end, just like our input tagged data) penn_dev_words is a python list where every element is a python list of the words of a particular sentence. taglist is a set of all possible tags known_words is a set of all known words transition_values is from the return of get_transitions_probs() e_values is from the return of calc_emissions() The return value is a list of tagged sentences in the format \"WORD/TAG\", separated by spaces. Each sentence is a string with a terminal newline, not a list of tokens. Remember also that the output should not contain the \"_RARE_\" symbol, but rather the original words of the sentence!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_viterbi(penn_dev_words, taglist, known_words, transition_values, e_values):\n",
    "    tagged = []\n",
    "\n",
    "    print(len(penn_dev_words))\n",
    "\n",
    "    Pi_Init = {}\n",
    "    for u in taglist:\n",
    "        for v in taglist:\n",
    "            Pi_Init[(u,v)] = LOG_PROB_OF_ZERO\n",
    "\n",
    "    for ctr, item in enumerate(penn_dev_words):\n",
    "\n",
    "        sentence = item + [STOP_SYMBOL]\n",
    "        converted_sentence = []\n",
    "        n = len(sentence)\n",
    "\n",
    "        cur_len = 0\n",
    "        Path_Pre = {} \n",
    "        Path_Pre[(START_SYMBOL, START_SYMBOL)] = [START_SYMBOL, START_SYMBOL]\n",
    "        Bigram_Pre = [(START_SYMBOL, START_SYMBOL)]           \n",
    "\n",
    "        Pi_Pre = {}\n",
    "        Pi_Pre[(START_SYMBOL, START_SYMBOL)] = 0\n",
    "        \n",
    "        # For each sentence\n",
    "        while cur_len < n:\n",
    "            Path_Cur = {}\n",
    "            Bigram_Cur = []\n",
    "            Pi_Cur = {}\n",
    "\n",
    "            if cur_len == n-1:\n",
    "                word = STOP_SYMBOL\n",
    "                tagspace = [STOP_SYMBOL]\n",
    "            else:\n",
    "                word = sentence[cur_len]\n",
    "                if word not in known_words:\n",
    "                    word = RARE_SYMBOL\n",
    "                tagspace = list(taglist)\n",
    "            converted_sentence.append(word)\n",
    "\n",
    "            for v in tagspace:\n",
    "                for u in taglist:\n",
    "                    emi_tmp = (word, v, u)\n",
    "                    if emi_tmp not in e_values:\n",
    "                        e_values[emi_tmp] = LOG_PROB_OF_ZERO\n",
    "                    w_tmp = ''\n",
    "                    for w in taglist:\n",
    "                        if (w,u) not in Bigram_Pre:\n",
    "                            continue\n",
    "                        trigram_cur = (w,u,v)\n",
    "                        if trigram_cur not in transition_values:\n",
    "                            transition_values[trigram_cur] = LOG_PROB_OF_ZERO\n",
    "                        if (u,v) not in Pi_Cur:\n",
    "                            Pi_Cur[(u,v)] = Pi_Pre[(w,u)]+transition_values[trigram_cur]+e_values[emi_tmp]\n",
    "                            w_tmp = w\n",
    "                        elif Pi_Pre[(w,u)]+transition_values[trigram_cur]+e_values[emi_tmp] > Pi_Cur[(u,v)]:\n",
    "                            Pi_Cur[(u,v)] = Pi_Pre[(w,u)]+transition_values[trigram_cur]+e_values[emi_tmp]\n",
    "                            w_tmp = w\n",
    "                    if w_tmp != '':\n",
    "                        Path_Cur[(u,v)] =  Path_Pre[(w_tmp,u)]+[v]\n",
    "                        Bigram_Cur.append((u,v))\n",
    "\n",
    "            Pi_Pre = dict(Pi_Cur)\n",
    "            Bigram_Pre = list(Bigram_Cur)\n",
    "            Path_Pre = dict(Path_Cur)\n",
    "            cur_len += 1\n",
    "\n",
    "        st = ''\n",
    "        bigram_max = Bigram_Pre[0]\n",
    "        for bigram in Bigram_Pre:\n",
    "            if Pi_Cur[bigram] > Pi_Cur[bigram_max]:\n",
    "                bigram = bigram_max\n",
    "        for i,tag in enumerate(Path_Cur[bigram_max][2:-1]):\n",
    "            st = st + sentence[i]+'_/'+tag+' '\n",
    "        tagged.append(st.strip()+'\\n')\n",
    "        if len(tagged) % 100 == 0:\n",
    "            print(len(tagged))\n",
    "\n",
    "    return tagged\n",
    "\n",
    "def s5_read(filename):\n",
    "    file = open(filename, 'r')\n",
    "    tagged = file.readline()\n",
    "    file.close()\n",
    "    return tagged\n",
    "\n",
    "# This function takes the output of do_viterbi() and outputs it to file\n",
    "def s5_output(tagged, filename):\n",
    "    tagged = s5_read(filename)\n",
    "    outfile = open(filename, 'w')\n",
    "    outfile.write(tagged)\n",
    "    outfile.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_devtags(dev_sentences):\n",
    "    penn_words = []\n",
    "    penn_tags = []\n",
    "\n",
    "    for item in dev_sentences:\n",
    "        words_tmp = []\n",
    "        tags_tmp = []\n",
    "        sentence = item.strip().split(' ')\n",
    "        for token in sentence:\n",
    "            words_tmp.append(token.rsplit('_/',1)[0])\n",
    "            tags_tmp.append(token.rsplit('_/',1)[1])\n",
    "        penn_words.append(words_tmp)\n",
    "        penn_tags.append(tags_tmp)\n",
    "\n",
    "    return penn_words, penn_tags\n",
    "\n",
    "def print_accuracy(test_penn_tags, test_tags, padding):\n",
    "    # Flattening all original tags\n",
    "    flat_orig_tags = []\n",
    "    for orig_tag in test_penn_tags:\n",
    "        flat_orig_tags.extend(orig_tag[2:-1])\n",
    "\n",
    "    # Flattening all original tags\n",
    "    reduce_orig_tags = []\n",
    "    for predict_tag in test_tags:\n",
    "        reduce_orig_tags.extend(predict_tag[2:-1])\n",
    "\n",
    "    assert len(flat_orig_tags) == len(reduce_orig_tags)\n",
    "    print(classification_report(flat_orig_tags, reduce_orig_tags))\n",
    "\n",
    "def change_tags(all_tags, choice):\n",
    "    reconstruct_tag = []\n",
    "    for taglist in all_tags:\n",
    "        new_tags = []\n",
    "        for tag in taglist:\n",
    "            split_tag = tag.split('-')\n",
    "            if len(split_tag) == 1:\n",
    "                new_tags.append(tag)\n",
    "            else:\n",
    "                if choice=='1':\n",
    "                    new_tags.append(split_tag[0])\n",
    "                else:\n",
    "                    new_tags.append(split_tag[1])\n",
    "        reconstruct_tag.append(new_tags)\n",
    "    return reconstruct_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # open penn training data\n",
    "    with open('./penn-data.json', 'r') as infile:\n",
    "        json_data = json.load(infile)\n",
    "\n",
    "    # split words and tags, and add start and stop symbols (question 1)\n",
    "    all_words, all_tags = read_data(json_data)\n",
    "    padding = 'result'\n",
    "    \n",
    "    # calculate tag trigram probabilities (question 2)\n",
    "    transition_values = get_transitions_probs(all_tags)\n",
    "\n",
    "    # calculate list of words with count > 5 (question 3)\n",
    "    known_words = calc_known(all_words)\n",
    "\n",
    "    # get a version of penn_words with rare words replace with '_RARE_' (question 3)\n",
    "    penn_words_rare = replace_rare(all_words, known_words)\n",
    "\n",
    "    # calculate emission probabilities (question 4)\n",
    "    e_values, taglist = get_emission_probs(penn_words_rare, all_tags)\n",
    "\n",
    "    # delete unneceessary data\n",
    "    del penn_words_rare\n",
    "\n",
    "    # Test Data\n",
    "    t_sentence = 'That former Sri Lanka skipper and ace batsman Aravinda De Silva is a man of few words was very much evident on Wednesday when the legendary batsman , who has always let his bat talk , struggled to answer a barrage of questions at a function to_F promote.'\n",
    "    t_words = t_sentence.split(' ')\n",
    "    test_put = [START_SYMBOL, START_SYMBOL]\n",
    "    test_put.extend(t_words)\n",
    "    test_put.append(STOP_SYMBOL)\n",
    "    test_data = [test_put]\n",
    "\n",
    "    tag_out = do_viterbi(test_data, taglist, known_words, transition_values, e_values)\n",
    "    s5_output(tag_out, 'tagged'+padding+'.txt')\n",
    "\n",
    "if __name__ == \"__main__\": \n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "46af86b762384d5b484041a46ecc7133789ecef2b2e62713afc23563f4b0cd95"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('nlp-midsem')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
